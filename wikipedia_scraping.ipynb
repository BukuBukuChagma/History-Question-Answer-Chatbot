{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Dependencies and Imports"
      ],
      "metadata": {
        "id": "gS8X66WgCBri"
      },
      "id": "gS8X66WgCBri"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "doCOy9TRw6iT"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install BeautifulSoup\n",
        "!pip install mwparserfromhell\n",
        "!pip install wikipedia"
      ],
      "id": "doCOy9TRw6iT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2810fb3b"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import mwparserfromhell\n",
        "from collections import deque\n",
        "import itertools\n",
        "import pandas as pd\n",
        "import wikipedia\n",
        "import random\n",
        "import time"
      ],
      "id": "2810fb3b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load FAQ Questions"
      ],
      "metadata": {
        "id": "eKCdYWGYCKOA"
      },
      "id": "eKCdYWGYCKOA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First download the spreadsheet containing the questions. These questions will be used to gather the intial wiki links to start with.<br>\n",
        "Download the FAQ spreadsheet : https://docs.google.com/spreadsheets/d/1l9yLalzMYzoQZaMc7P3is446En0EzFyeAAJO5Jm1ENA/edit?usp=sharing<br>"
      ],
      "metadata": {
        "id": "c4R2btUzDfbd"
      },
      "id": "c4R2btUzDfbd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhjOUanrTpo5"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/Most Frequently Asked Questions In Nutrition Domain.csv')\n",
        "queries = df['Questions']"
      ],
      "id": "BhjOUanrTpo5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gather Intial Wiki Links"
      ],
      "metadata": {
        "id": "9e6oWVLYD75x"
      },
      "id": "9e6oWVLYD75x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since sending large number of request to google can trigger the ban hammer, we are going to use free available proxies to avoid that.<br> We load up proxies from https://sslproxies.org/<br>"
      ],
      "metadata": {
        "id": "x2_haxoeD-OA"
      },
      "id": "x2_haxoeD-OA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRhD-nj60V28"
      },
      "outputs": [],
      "source": [
        "def LoadUpProxies():\n",
        "\turl='https://sslproxies.org/'\n",
        "\tresponse=requests.get(url)\n",
        "\tsoup=BeautifulSoup(response.content, 'lxml')\n",
        "\treturn [p for p in soup.select('textarea')[0].contents[0].split('\\n\\n')[1].split('\\n') if not p == '']\n",
        "proxyBuffer = itertools.cycle(LoadUpProxies())"
      ],
      "id": "eRhD-nj60V28"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use the questions from FAQ spreadsheet to gather intial wiki links. <br> We also save iteratively incase a problem arises while in the middle of execution. So, we don't have to start from scratch again."
      ],
      "metadata": {
        "id": "l26ttqKFFCyG"
      },
      "id": "l26ttqKFFCyG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IapXiSK6QuS8"
      },
      "outputs": [],
      "source": [
        "wiki_titles = []\n",
        "count = 0\n",
        "\n",
        "for query in queries:\n",
        "  print(f\"Question Count : {count}\")\n",
        "  query = \"https://www.google.com/search?q=\" + query + ' wikipedia '\n",
        "  while True:\n",
        "      proxy = proxyBuffer.__next__()\n",
        "      try:\n",
        "         time.sleep(2)\n",
        "         page = requests.get(url=query, proxies={\"http\": proxy})\n",
        "         soup = BeautifulSoup(page.content, 'html.parser')\n",
        "         allLinks = soup.find_all(\"a\")\n",
        "         for link in allLinks:\n",
        "             link = link.get('href')\n",
        "             if link is not None:\n",
        "                 if link.find(\"/url?q=https://en.wikipedia.org/wiki/\") == -1:\n",
        "                     continue\n",
        "                 page_title = link[link.find(\"/wiki/\")+6:]\n",
        "                 page_title = page_title[:page_title.find(\"&sa\")]\n",
        "                 wiki_titles.append(page_title)\n",
        "         count+=1\n",
        "         break\n",
        "      except:\n",
        "        print(f'Proxy failed: {proxy}') # proxy failed, try the next one\n",
        "    \n",
        "  if count % 10 == 0:\n",
        "    with open('wiki_titles.txt', 'w') as f:\n",
        "        for line in wiki_titles:\n",
        "            f.write(f\"{line}\\n\")"
      ],
      "id": "IapXiSK6QuS8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code can be used to read the saved **wiki_titles.txt** file above."
      ],
      "metadata": {
        "id": "ESC0nzZjFn3I"
      },
      "id": "ESC0nzZjFn3I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59gtgqQy8yf5"
      },
      "outputs": [],
      "source": [
        "# For reading the saved titles from the file\n",
        "\n",
        "# wiki_titles = list()\n",
        "# with open('wiki_titles.txt', 'r') as wiki_read_file:\n",
        "#   wiki_titles = wiki_read_file.read().split(\"\\n\")"
      ],
      "id": "59gtgqQy8yf5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrape Wikipedia\n",
        "Using the gathered wiki links, we now scrape the data from those links and also scrape the links present on that page. And the cycle continues untill we reach the max article we need or there are no more links.<br><br>\n",
        "**Note:** Incase the connection to notebook is lost, we don't wanna lose all the progress we have made. So, we are iteratively saving data to google drive. You can opt to save them to your own google drive, or if you are on pc then just put your local path."
      ],
      "metadata": {
        "id": "8XfPKyXdGIpK"
      },
      "id": "8XfPKyXdGIpK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4rwzml_OJUB"
      },
      "outputs": [],
      "source": [
        "title_queue = deque(wiki_titles)\n",
        "max_articles_to_retrieve = 10000\n",
        "count = 0\n",
        "data = []\n",
        "visited_titles = set()\n",
        "nonexistant_pages = set()\n",
        "\n",
        "while title_queue or len(visited_titles) < max_articles_to_retrieve:\n",
        "  title = title_queue.popleft()\n",
        "\n",
        "  try:\n",
        "     page = wikipedia.page(title, auto_suggest=False)\n",
        "  except wikipedia.DisambiguationError as e: \n",
        "     #Sometimes the title is ambiguous so wikipedia instead returns us with options we can choose with. We choose randomly\n",
        "     s = random.choice(e.options)\n",
        "     try:\n",
        "         #Sometimes even the title in options is ambiguous. If the page is found good, otherwise we just insert it in nonexistant_pages\n",
        "         page = wikipedia.page(s, auto_suggest=False)\n",
        "     except:\n",
        "         nonexistant_pages.add(title)\n",
        "         continue\n",
        "  except:\n",
        "     nonexistant_pages.add(title)\n",
        "     continue\n",
        "  \n",
        "  visited_titles.add(title)\n",
        "  \n",
        "  try:\n",
        "    data.append(page.content)\n",
        "  except:\n",
        "    a = \"Do nothing\"\n",
        "\n",
        "  count+=1\n",
        "  print(f\"Pages Scrapped Count : {count}\")\n",
        "\n",
        "  try:\n",
        "    more_titles = set(page.links)\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "  more_titles = more_titles - visited_titles\n",
        "  title_queue.extend(more_titles)\n",
        "\n",
        "  if count % 50 == 0:\n",
        "\n",
        "    #----------Change these paths to your paths---------------\n",
        "\n",
        "    with open('/content/drive/MyDrive/ColabData/wiki_data/wikidata.txt', 'w') as wikiFile:\n",
        "      wikiFile.write(json.dumps(data))\n",
        "      wikiFile.close()\n",
        "\n",
        "    with open('/content/drive/MyDrive/ColabData/wiki_data/wiki_titles_extended.txt', 'w') as f:\n",
        "      for line in list(title_queue)[1:]:\n",
        "          f.write(f\"{line}\\n\")\n",
        "    print(\"Writing to file...Done\")"
      ],
      "id": "g4rwzml_OJUB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can load the saved data using the code below in notebooks where this is needed."
      ],
      "metadata": {
        "id": "rP_C2RevGwYB"
      },
      "id": "rP_C2RevGwYB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFiFWzSGgkaP"
      },
      "outputs": [],
      "source": [
        "# For Loading up saved data from google drive\n",
        "with open('/content/drive/MyDrive/ColabData/wiki_data/wikidata.txt', 'r') as wikidata:\n",
        "  data = json.load(wikidata)"
      ],
      "id": "SFiFWzSGgkaP"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
